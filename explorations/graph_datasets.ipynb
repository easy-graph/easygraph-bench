{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import datasets\n",
    "\n",
    "# add .. to pythonpath\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset_loaders import *\n",
    "from utils import eg2nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "loading graph cheminformatics ...\n",
      "################################################################################\n",
      "################################################################################\n",
      "finished loading graph cheminformatics\n",
      "ENZYMES_g1: nodes: 37 edges: 168 is_directed: True\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x11d3e7b20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cd to ..\n",
    "import os\n",
    "os.chdir('/Users/tscp/testdir/easygraph-bench/')\n",
    "chem = eg2nx(load_cheminformatics())\n",
    "chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 1 {}',\n",
       " '2 3 {}',\n",
       " '2 4 {}',\n",
       " '2 25 {}',\n",
       " '2 28 {}',\n",
       " '1 2 {}',\n",
       " '1 3 {}',\n",
       " '1 4 {}',\n",
       " '3 1 {}',\n",
       " '3 2 {}']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(nx.generate_edgelist(chem), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /Users/tscp/testdir/easygraph-bench/easygraph-bench/cheminformatics/cheminformatics.py or any data file in the same directory. Couldn't find 'easygraph-bench/cheminformatics' on the Hugging Face Hub either: FileNotFoundError: Unable to resolve any data file that matches ['**'] in dataset repository easygraph-bench/cheminformatics with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'BLP', 'BMP', 'DIB', 'BUFR', 'CUR', 'PCX', 'DCX', 'DDS', 'PS', 'EPS', 'FIT', 'FITS', 'FLI', 'FLC', 'FTC', 'FTU', 'GBR', 'GIF', 'GRIB', 'H5', 'HDF', 'PNG', 'APNG', 'JP2', 'J2K', 'JPC', 'JPF', 'JPX', 'J2C', 'ICNS', 'ICO', 'IM', 'IIM', 'TIF', 'TIFF', 'JFIF', 'JPE', 'JPG', 'JPEG', 'MPG', 'MPEG', 'MSP', 'PCD', 'PXR', 'PBM', 'PGM', 'PPM', 'PNM', 'PSD', 'BW', 'RGB', 'RGBA', 'SGI', 'RAS', 'TGA', 'ICB', 'VDA', 'VST', 'WEBP', 'WMF', 'EMF', 'XBM', 'XPM', 'aiff', 'au', 'avr', 'caf', 'flac', 'htk', 'svx', 'mat4', 'mat5', 'mpc2k', 'ogg', 'paf', 'pvf', 'raw', 'rf64', 'sd2', 'sds', 'ircam', 'voc', 'w64', 'wav', 'nist', 'wavex', 'wve', 'xi', 'mp3', 'opus', 'AIFF', 'AU', 'AVR', 'CAF', 'FLAC', 'HTK', 'SVX', 'MAT4', 'MAT5', 'MPC2K', 'OGG', 'PAF', 'PVF', 'RAW', 'RF64', 'SD2', 'SDS', 'IRCAM', 'VOC', 'W64', 'WAV', 'NIST', 'WAVEX', 'WVE', 'XI', 'MP3', 'OPUS', 'zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# load https://huggingface.co/datasets/easygraph-bench/cheminformatics from huggingface hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# c = \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m c \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m'\u001b[39;49m\u001b[39measygraph-bench/cheminformatics\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# c\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1760\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1761\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1762\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1763\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1764\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1765\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1766\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1767\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1768\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1769\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1770\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1497\u001b[0m     path,\n\u001b[1;32m   1498\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1500\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1501\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1502\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1503\u001b[0m )\n\u001b[1;32m   1505\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/load.py:1214\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1213\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1214\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1218\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1219\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /Users/tscp/testdir/easygraph-bench/easygraph-bench/cheminformatics/cheminformatics.py or any data file in the same directory. Couldn't find 'easygraph-bench/cheminformatics' on the Hugging Face Hub either: FileNotFoundError: Unable to resolve any data file that matches ['**'] in dataset repository easygraph-bench/cheminformatics with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'BLP', 'BMP', 'DIB', 'BUFR', 'CUR', 'PCX', 'DCX', 'DDS', 'PS', 'EPS', 'FIT', 'FITS', 'FLI', 'FLC', 'FTC', 'FTU', 'GBR', 'GIF', 'GRIB', 'H5', 'HDF', 'PNG', 'APNG', 'JP2', 'J2K', 'JPC', 'JPF', 'JPX', 'J2C', 'ICNS', 'ICO', 'IM', 'IIM', 'TIF', 'TIFF', 'JFIF', 'JPE', 'JPG', 'JPEG', 'MPG', 'MPEG', 'MSP', 'PCD', 'PXR', 'PBM', 'PGM', 'PPM', 'PNM', 'PSD', 'BW', 'RGB', 'RGBA', 'SGI', 'RAS', 'TGA', 'ICB', 'VDA', 'VST', 'WEBP', 'WMF', 'EMF', 'XBM', 'XPM', 'aiff', 'au', 'avr', 'caf', 'flac', 'htk', 'svx', 'mat4', 'mat5', 'mpc2k', 'ogg', 'paf', 'pvf', 'raw', 'rf64', 'sd2', 'sds', 'ircam', 'voc', 'w64', 'wav', 'nist', 'wavex', 'wve', 'xi', 'mp3', 'opus', 'AIFF', 'AU', 'AVR', 'CAF', 'FLAC', 'HTK', 'SVX', 'MAT4', 'MAT5', 'MPC2K', 'OGG', 'PAF', 'PVF', 'RAW', 'RF64', 'SD2', 'SDS', 'IRCAM', 'VOC', 'W64', 'WAV', 'NIST', 'WAVEX', 'WVE', 'XI', 'MP3', 'OPUS', 'zip']"
     ]
    }
   ],
   "source": [
    "# load https://huggingface.co/datasets/easygraph-bench/cheminformatics from huggingface hub\n",
    "# c = \n",
    "c = datasets.load_dataset('easygraph-bench/cheminformatics')\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chem.edges[('2', '1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def convert_graph_to_jsonl(graph: nx.Graph | nx.DiGraph, filename):\n",
    "    is_directed = isinstance(graph, nx.DiGraph)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps({'directed': is_directed}) + '\\n')\n",
    "        for node in graph.nodes:\n",
    "            data = {'id': node}\n",
    "            for key, value in graph.nodes[node].items():\n",
    "                data[key] = value\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "        for edge in graph.edges:\n",
    "            data = {'source': edge[0], 'target': edge[1]}\n",
    "            for key, value in graph.edges[edge].items():\n",
    "                data[key] = value\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def create_graph_from_jsonl(filename, directed: bool | None = None):\n",
    "    def get_is_directed() -> bool | None:\n",
    "        # read the first line to determine if the graph is directed\n",
    "        is_directed = None\n",
    "        with open(filename, 'r') as f:\n",
    "            line = f.readline()\n",
    "            data = json.loads(line.strip())\n",
    "            if 'directed' in data:\n",
    "                is_directed = data['directed']\n",
    "        return is_directed\n",
    "    # check if directed is set, if not set, call get_is_directed to get is_directed, if is_directed is None, defaults to False\n",
    "    if directed is None:\n",
    "        directed = get_is_directed()\n",
    "        if directed is None:\n",
    "            directed = False\n",
    "\n",
    "    if directed:\n",
    "        graph = nx.DiGraph()\n",
    "    else:\n",
    "        graph = nx.Graph()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            if 'id' in data:\n",
    "                id = data.pop('id')\n",
    "                graph.add_node(id, **data)\n",
    "            elif 'source' in data and 'target' in data:\n",
    "                source = data.pop('source')\n",
    "                target = data.pop('target')\n",
    "                graph.add_edge(source, target, **data)\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import networkx as nx\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "\n",
    "def load_dataset(filename: str, directed: bool = False) -> Dict[str, Dataset]:\n",
    "    # Define the features for the dataset\n",
    "    features = Features({\n",
    "        'graph': {'nodes': Features({'attr': Dict([Value('string')])}),\n",
    "                  'edges': Features({'source': Value('int64'),\n",
    "                                     'target': Value('int64'),\n",
    "                                     'attr': Dict(Value('string'))})}\n",
    "    })\n",
    "\n",
    "    # Load the JSONL file into a NetworkX graph\n",
    "    graph = create_graph_from_jsonl(filename, directed=directed)\n",
    "\n",
    "    # Convert the graph to a dictionary format that matches the features\n",
    "    nodes = [{'id': node, 'attr': attrs} for node, attrs in graph.nodes(data=True)]\n",
    "    edges = [{'source': u, 'target': v, 'attr': attrs} for u, v, attrs in graph.edges(data=True)]\n",
    "    data = {'graph': {'nodes': nodes, 'edges': edges}}\n",
    "\n",
    "    # Create a dataset from the dictionary data using the defined features\n",
    "    dataset = Dataset.from_dict(data, features=features)\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset = dataset.shuffle().select(range(100))\n",
    "    valid_dataset = dataset.shuffle().select(range(100, 150))\n",
    "    test_dataset = dataset.shuffle().select(range(150, 200))\n",
    "\n",
    "    # Return a DatasetDict object with the splits\n",
    "    return {'train': train_dataset, 'validation': valid_dataset, 'test': test_dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x166886410>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_graph_to_jsonl(chem, 'chem.jsonl')\n",
    "chem2 = create_graph_from_jsonl('chem.jsonl')\n",
    "chem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# check if chem and chem2 are the same\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnetworkx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m nodes_equal, edges_equal\n\u001b[0;32m----> 3\u001b[0m nodes_equal(chem, chem2) \u001b[39mand\u001b[39;00m edges_equal(chem, chem2)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/networkx/utils/misc.py:442\u001b[0m, in \u001b[0;36medges_equal\u001b[0;34m(edges1, edges2)\u001b[0m\n\u001b[1;32m    440\u001b[0m c1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[39mfor\u001b[39;00m c1, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(edges1):\n\u001b[0;32m--> 442\u001b[0m     u, v \u001b[39m=\u001b[39m e[\u001b[39m0\u001b[39m], e[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m    443\u001b[0m     data \u001b[39m=\u001b[39m [e[\u001b[39m2\u001b[39m:]]\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m v \u001b[39min\u001b[39;00m d1[u]:\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# check if chem and chem2 are the same\n",
    "from networkx.utils import nodes_equal, edges_equal\n",
    "nodes_equal(chem, chem2) and edges_equal(chem, chem2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OutEdgeView([('2', '1'), ('2', '3'), ('2', '4'), ('2', '25'), ('2', '28'), ('1', '2'), ('1', '3'), ('1', '4'), ('3', '1'), ('3', '2'), ('3', '4'), ('3', '28'), ('3', '29'), ('4', '1'), ('4', '2'), ('4', '3'), ('4', '5'), ('4', '6'), ('4', '29'), ('25', '2'), ('25', '28'), ('25', '29'), ('25', '30'), ('28', '2'), ('28', '3'), ('28', '25'), ('28', '29'), ('28', '30'), ('29', '3'), ('29', '4'), ('29', '25'), ('29', '27'), ('29', '28'), ('29', '30'), ('5', '4'), ('5', '6'), ('5', '7'), ('5', '30'), ('6', '4'), ('6', '5'), ('6', '7'), ('6', '8'), ('6', '30'), ('30', '5'), ('30', '6'), ('30', '25'), ('30', '26'), ('30', '27'), ('30', '28'), ('30', '29'), ('27', '12'), ('27', '13'), ('27', '26'), ('27', '29'), ('27', '30'), ('7', '5'), ('7', '6'), ('7', '8'), ('7', '9'), ('8', '6'), ('8', '7'), ('8', '9'), ('8', '10'), ('8', '11'), ('9', '7'), ('9', '8'), ('9', '10'), ('26', '13'), ('26', '14'), ('26', '15'), ('26', '27'), ('26', '30'), ('10', '8'), ('10', '9'), ('10', '11'), ('10', '12'), ('10', '13'), ('11', '8'), ('11', '10'), ('11', '12'), ('11', '13'), ('12', '10'), ('12', '11'), ('12', '13'), ('12', '27'), ('13', '10'), ('13', '11'), ('13', '12'), ('13', '26'), ('13', '27'), ('14', '15'), ('14', '16'), ('14', '17'), ('14', '26'), ('15', '14'), ('15', '16'), ('15', '17'), ('15', '26'), ('16', '14'), ('16', '15'), ('16', '17'), ('16', '18'), ('17', '14'), ('17', '15'), ('17', '16'), ('17', '18'), ('18', '16'), ('18', '17'), ('18', '19'), ('18', '20'), ('19', '18'), ('19', '20'), ('19', '21'), ('20', '18'), ('20', '19'), ('20', '21'), ('21', '19'), ('21', '20'), ('21', '22'), ('21', '23'), ('21', '24'), ('21', '31'), ('22', '21'), ('22', '23'), ('22', '24'), ('22', '31'), ('22', '36'), ('23', '21'), ('23', '22'), ('23', '24'), ('23', '36'), ('24', '21'), ('24', '22'), ('24', '23'), ('24', '34'), ('31', '21'), ('31', '22'), ('31', '34'), ('31', '35'), ('31', '36'), ('36', '22'), ('36', '23'), ('36', '31'), ('36', '34'), ('36', '35'), ('34', '24'), ('34', '31'), ('34', '33'), ('34', '35'), ('34', '36'), ('34', '37'), ('35', '31'), ('35', '32'), ('35', '33'), ('35', '34'), ('35', '36'), ('35', '37'), ('33', '32'), ('33', '34'), ('33', '35'), ('33', '37'), ('37', '32'), ('37', '33'), ('37', '34'), ('37', '35'), ('32', '33'), ('32', '35'), ('32', '37')]),\n",
       " OutEdgeView([('2', '1'), ('2', '3'), ('2', '4'), ('2', '25'), ('2', '28'), ('1', '2'), ('1', '3'), ('1', '4'), ('3', '1'), ('3', '2'), ('3', '4'), ('3', '28'), ('3', '29'), ('4', '1'), ('4', '2'), ('4', '3'), ('4', '5'), ('4', '6'), ('4', '29'), ('25', '2'), ('25', '28'), ('25', '29'), ('25', '30'), ('28', '2'), ('28', '3'), ('28', '25'), ('28', '29'), ('28', '30'), ('29', '3'), ('29', '4'), ('29', '25'), ('29', '27'), ('29', '28'), ('29', '30'), ('5', '4'), ('5', '6'), ('5', '7'), ('5', '30'), ('6', '4'), ('6', '5'), ('6', '7'), ('6', '8'), ('6', '30'), ('30', '5'), ('30', '6'), ('30', '25'), ('30', '26'), ('30', '27'), ('30', '28'), ('30', '29'), ('27', '12'), ('27', '13'), ('27', '26'), ('27', '29'), ('27', '30'), ('7', '5'), ('7', '6'), ('7', '8'), ('7', '9'), ('8', '6'), ('8', '7'), ('8', '9'), ('8', '10'), ('8', '11'), ('9', '7'), ('9', '8'), ('9', '10'), ('26', '13'), ('26', '14'), ('26', '15'), ('26', '27'), ('26', '30'), ('10', '8'), ('10', '9'), ('10', '11'), ('10', '12'), ('10', '13'), ('11', '8'), ('11', '10'), ('11', '12'), ('11', '13'), ('12', '10'), ('12', '11'), ('12', '13'), ('12', '27'), ('13', '10'), ('13', '11'), ('13', '12'), ('13', '26'), ('13', '27'), ('14', '15'), ('14', '16'), ('14', '17'), ('14', '26'), ('15', '14'), ('15', '16'), ('15', '17'), ('15', '26'), ('16', '14'), ('16', '15'), ('16', '17'), ('16', '18'), ('17', '14'), ('17', '15'), ('17', '16'), ('17', '18'), ('18', '16'), ('18', '17'), ('18', '19'), ('18', '20'), ('19', '18'), ('19', '20'), ('19', '21'), ('20', '18'), ('20', '19'), ('20', '21'), ('21', '19'), ('21', '20'), ('21', '22'), ('21', '23'), ('21', '24'), ('21', '31'), ('22', '21'), ('22', '23'), ('22', '24'), ('22', '31'), ('22', '36'), ('23', '21'), ('23', '22'), ('23', '24'), ('23', '36'), ('24', '21'), ('24', '22'), ('24', '23'), ('24', '34'), ('31', '21'), ('31', '22'), ('31', '34'), ('31', '35'), ('31', '36'), ('36', '22'), ('36', '23'), ('36', '31'), ('36', '34'), ('36', '35'), ('34', '24'), ('34', '31'), ('34', '33'), ('34', '35'), ('34', '36'), ('34', '37'), ('35', '31'), ('35', '32'), ('35', '33'), ('35', '34'), ('35', '36'), ('35', '37'), ('33', '32'), ('33', '34'), ('33', '35'), ('33', '37'), ('37', '32'), ('37', '33'), ('37', '34'), ('37', '35'), ('32', '33'), ('32', '35'), ('32', '37')]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chem.nodes), len(chem.edges), len(chem2.nodes), len(chem2.edges)\n",
    "chem.nodes, chem2.nodes\n",
    "chem.edges, chem2.edges\n",
    "# chem.nodes['1'], chem2.nodes['1']\n",
    "# chem.edges[('2', '1')], chem2.edges[('2', '1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08221793174743652,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 5274,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434d450d78e841afa75aa4e02e5b3410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008655309677124023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading metadata",
       "rate": null,
       "total": 2360,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237190a10b67454dbbcc6b57bf320471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022281885147094727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 7665,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bce5b57ea34129be880dc952e51ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to /Users/tscp/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008999824523925781,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400d2a5e29cd47be87d9609256aa38fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011411905288696289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 8116577,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c7deab93fe46159b9c6f85fa795ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009294986724853516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 1054280,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c23c1df6444197bf3fd6619e5d1f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013601064682006836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40c5a8b691e440fa7f9cd568a00c465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027309656143188477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 87599,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902f3ec935e4ebe81601e298e628002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018158912658691406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating validation split",
       "rate": null,
       "total": 10570,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f619dbeb2ff4983b9821b19f8787277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/tscp/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00982809066772461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8518c30706d04eaaabe189b049a76670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "s = squad = load_dataset('squad')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SplitGenerator(name='train', gen_kwargs={'filepath': '/Users/tscp/testdir/test/chem.tsv'}, split_info=SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name=None)),)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": \"/Users/tscp/testdir/test/chem.tsv\"}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sg \u001b[39m=\u001b[39m _\n\u001b[0;32m----> 2\u001b[0m \u001b[39mlist\u001b[39m(sg())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "sg = _\n",
    "list(sg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 2\u001b[0m ds \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_generator(nx\u001b[39m.\u001b[39;49mgenerate_edgelist(chem))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/arrow_dataset.py:1012\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerator\u001b[39;00m \u001b[39mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[0;32m-> 1012\u001b[0m \u001b[39mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[1;32m   1013\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m   1014\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1015\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1016\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1017\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m   1018\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1019\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1020\u001b[0m )\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/io/generator.py:28\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.__init__\u001b[0;34m(self, generator, features, cache_dir, keep_in_memory, streaming, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     10\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     11\u001b[0m     generator: Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     19\u001b[0m ):\n\u001b[1;32m     20\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     21\u001b[0m         features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m     22\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder \u001b[39m=\u001b[39m Generator(\n\u001b[1;32m     29\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m     30\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m     31\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m     32\u001b[0m         gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m     33\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     34\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:1396\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1396\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1397\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:319\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    320\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    321\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    322\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:487\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBuilderConfig must have a name, got \u001b[39m\u001b[39m{\u001b[39;00mbuilder_config\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m \u001b[39m# compute the config id that is going to be used for caching\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m config_id \u001b[39m=\u001b[39m builder_config\u001b[39m.\u001b[39;49mcreate_config_id(\n\u001b[1;32m    488\u001b[0m     config_kwargs,\n\u001b[1;32m    489\u001b[0m     custom_features\u001b[39m=\u001b[39;49mcustom_features,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m is_custom \u001b[39m=\u001b[39m (config_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs) \u001b[39mand\u001b[39;00m config_id \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m is_custom:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:179\u001b[0m, in \u001b[0;36mBuilderConfig.create_config_id\u001b[0;34m(self, config_kwargs, custom_features)\u001b[0m\n\u001b[1;32m    177\u001b[0m             suffix \u001b[39m=\u001b[39m Hasher\u001b[39m.\u001b[39mhash(config_kwargs_to_add_to_suffix)\n\u001b[1;32m    178\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         suffix \u001b[39m=\u001b[39m Hasher\u001b[39m.\u001b[39;49mhash(config_kwargs_to_add_to_suffix)\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m custom_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     m \u001b[39m=\u001b[39m Hasher()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/fingerprint.py:236\u001b[0m, in \u001b[0;36mHasher.hash\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdispatch[\u001b[39mtype\u001b[39m(value)](\u001b[39mcls\u001b[39m, value)\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mhash_default(value)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/fingerprint.py:229\u001b[0m, in \u001b[0;36mHasher.hash_default\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhash_default\u001b[39m(\u001b[39mcls\u001b[39m, value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mhash_bytes(dumps(value))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/utils/py_utils.py:722\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    720\u001b[0m file \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    721\u001b[0m \u001b[39mwith\u001b[39;00m _no_cache_fields(obj):\n\u001b[0;32m--> 722\u001b[0m     dump(obj, file)\n\u001b[1;32m    723\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/utils/py_utils.py:697\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file):\n\u001b[1;32m    696\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"pickle an object to a file\"\"\"\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m     Pickler(file, recurse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    698\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/dill/_dill.py:394\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     logger\u001b[39m.\u001b[39mtrace_setup(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/utils/py_utils.py:687\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m dill\u001b[39m.\u001b[39;49mPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id\u001b[39m=\u001b[39;49msave_persistent_id)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/dill/_dill.py:1186\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1187\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/utils/py_utils.py:687\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m dill\u001b[39m.\u001b[39;49mPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id\u001b[39m=\u001b[39;49msave_persistent_id)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_generator(nx.generate_edgelist(chem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /Users/tscp/.cache/huggingface/datasets/generator/default-65bcf112e25345aa/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022563934326171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1452a70439408c9cc34d23a5f26815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:1635\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1634\u001b[0m num_shards \u001b[39m=\u001b[39m shard_id \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1635\u001b[0m num_examples, num_bytes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[1;32m   1636\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/arrow_writer.py:582\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 582\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[1;32m    583\u001b[0m \u001b[39m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/arrow_writer.py:431\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m# order the columns properly\u001b[39;00m\n\u001b[1;32m    427\u001b[0m cols \u001b[39m=\u001b[39m (\n\u001b[1;32m    428\u001b[0m     [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mnames \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m    429\u001b[0m     \u001b[39m+\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mnames]\n\u001b[1;32m    430\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[0;32m--> 431\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_examples[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mkeys()\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    433\u001b[0m batch_examples \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[39myield\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mpokemon\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbulbasaur\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgrass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      5\u001b[0m     \u001b[39myield\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mpokemon\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msquirtle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mwater\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m----> 6\u001b[0m ds \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_generator(gen)\n\u001b[1;32m      7\u001b[0m ds[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/arrow_dataset.py:1020\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerator\u001b[39;00m \u001b[39mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1012\u001b[0m \u001b[39mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[1;32m   1013\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m   1014\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1015\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1016\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1017\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m   1018\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1019\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m-> 1020\u001b[0m )\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/io/generator.py:47\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     verification_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     base_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m     48\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m     49\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m     50\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m     51\u001b[0m         \u001b[39m# try_from_hf_gcs=try_from_hf_gcs,\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m         base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m     53\u001b[0m         num_proc\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mas_dataset(\n\u001b[1;32m     56\u001b[0m         split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, verification_mode\u001b[39m=\u001b[39mverification_mode, in_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_in_memory\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:872\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 872\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    873\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    874\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    875\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    876\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    877\u001b[0m     )\n\u001b[1;32m    878\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1650\u001b[0m         dl_manager,\n\u001b[1;32m   1651\u001b[0m         verification_mode,\n\u001b[1;32m   1652\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1653\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1654\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:967\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m    965\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m    968\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    970\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    972\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    973\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    974\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:1488\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1486\u001b[0m gen_kwargs \u001b[39m=\u001b[39m split_generator\u001b[39m.\u001b[39mgen_kwargs\n\u001b[1;32m   1487\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1488\u001b[0m \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1489\u001b[0m     gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1490\u001b[0m ):\n\u001b[1;32m   1491\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   1492\u001b[0m         result \u001b[39m=\u001b[39m content\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:1644\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1643\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[0;32m-> 1644\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def gen():\n",
    "    yield 'a b'\n",
    "    yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n",
    "    yield {\"pokemon\": \"squirtle\", \"type\": \"water\"}\n",
    "ds = Dataset.from_generator(gen)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidConfigName",
     "evalue": "Bad characters from black list '<>:/\\|?*' found in '/Users/tscp/testdir/easygraph-bench/chem.jsonl'. They could create issues when creating a directory for this config on Windows filesystem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidConfigName\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m/Users/tscp/testdir/easygraph-bench/chem.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1760\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1761\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1762\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1763\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1764\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1765\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1766\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1767\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1768\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1769\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1770\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/load.py:1522\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1521\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1523\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1524\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m   1525\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1526\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1527\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1528\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1529\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1530\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1532\u001b[0m )\n\u001b[1;32m   1534\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:319\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    320\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    321\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    322\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:472\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m config_kwargs \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERSION\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION:\n\u001b[1;32m    471\u001b[0m         config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION\n\u001b[0;32m--> 472\u001b[0m     builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBUILDER_CONFIG_CLASS(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs)\n\u001b[1;32m    474\u001b[0m \u001b[39m# otherwise use the config_kwargs to overwrite the attributes\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     builder_config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(builder_config)\n",
      "File \u001b[0;32m<string>:14\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, name, version, data_dir, data_files, description, features, field, use_threads, block_size, chunksize, newlines_in_values)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/eg/lib/python3.10/site-packages/datasets/builder.py:125\u001b[0m, in \u001b[0;36mBuilderConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m invalid_char \u001b[39min\u001b[39;00m INVALID_WINDOWS_CHARACTERS_IN_PATH:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m invalid_char \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname:\n\u001b[0;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidConfigName(\n\u001b[1;32m    126\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBad characters from black list \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mINVALID_WINDOWS_CHARACTERS_IN_PATH\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m found in \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThey could create issues when creating a directory for this config on Windows filesystem.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files, DataFilesDict):\n\u001b[1;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a DataFilesDict in data_files but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidConfigName\u001b[0m: Bad characters from black list '<>:/\\|?*' found in '/Users/tscp/testdir/easygraph-bench/chem.jsonl'. They could create issues when creating a directory for this config on Windows filesystem."
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', '/Users/tscp/testdir/easygraph-bench/chem.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
